{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>hair_color</th>\n",
       "      <th>eyeglasses</th>\n",
       "      <th>smiling</th>\n",
       "      <th>young</th>\n",
       "      <th>human</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>2531</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4401</th>\n",
       "      <td>4830</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>907</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>522</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      file_name  hair_color  eyeglasses  smiling  young  human\n",
       "2308       2531           2           1        1      1     -1\n",
       "82           92           1           1        1      1     -1\n",
       "4401       4830          -1          -1       -1      1      1\n",
       "817         907           0          -1       -1      1      1\n",
       "471         522           1          -1        1      1      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%store -r \n",
    "#restore train, validate, and test dataframes from other notebook\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_images_to_main_dir(delete_dirs=True):\n",
    "    dest_dir = './images/'\n",
    "    walker = os.walk(dest_dir)\n",
    "    rem_dirs = walker.__next__()[1]\n",
    " \n",
    "    for data in walker:\n",
    "        for files in data[2]:\n",
    "            try:\n",
    "                shutil.move(data[0] + os.sep + files, dest_dir)\n",
    "            except shutil.Error:\n",
    "                print(shutil.Error)\n",
    "                continue\n",
    "    if delete_dirs:\n",
    "        for dirs in rem_dirs:\n",
    "            shutil.rmtree(dest_dir + os.sep + dirs)\n",
    "\n",
    "def symlink_classes_images(train, validate, test, reset_images=False, \n",
    "                           delete_dirs=False):\n",
    "    if reset_images:\n",
    "        reset_images_to_main_dir(delete_dirs=delete_dirs)\n",
    "    imagesPath = os.path.abspath('./images/')\n",
    "    trainPath = os.path.abspath('./images/train/eyeglasses/')\n",
    "    validatePath = os.path.abspath('./images/validate/eyeglasses/')\n",
    "    testPath = os.path.abspath('./images/test/eyeglasses/')\n",
    "    newPaths = [trainPath, validatePath, testPath]\n",
    "    dfList = [train, validate, test]\n",
    "    for path in newPaths:\n",
    "        if os.path.isdir(path)==False:\n",
    "            os.mkdir(path)\n",
    "    \n",
    "    for df in dfList:\n",
    "        for idx, row in df.iterrows():\n",
    "            if int(row['eyeglasses'])==-1:\n",
    "                pathAppend = 'no_glasses/'\n",
    "            elif int(row['eyeglasses']==1):\n",
    "                pathAppend = 'glasses/'\n",
    "            fileName = str(int(row['file_name']))+'.png'\n",
    "            filePath = os.path.join(imagesPath, fileName)\n",
    "            trainClassPath = os.path.join(trainPath, pathAppend)\n",
    "            trainFilePath = os.path.join(trainClassPath, fileName)\n",
    "            validateClassPath = os.path.join(validatePath, pathAppend)\n",
    "            validateFilePath = os.path.join(validateClassPath, fileName)\n",
    "            testClassPath = os.path.join(testPath, pathAppend)\n",
    "            testFilePath = os.path.join(testClassPath, fileName)\n",
    "            classPaths = [trainClassPath, validateClassPath, testClassPath]\n",
    "            for path in classPaths:\n",
    "                if os.path.isdir(path)==False:\n",
    "                    os.mkdir(path)\n",
    "            if(df.equals(test)):\n",
    "                newFilePath = testFilePath\n",
    "            if(df.equals(train)):\n",
    "                newFilePath = trainFilePath\n",
    "            if(df.equals(validate)):\n",
    "                newFilePath = validateFilePath\n",
    "            if os.path.isfile(filePath):\n",
    "                os.symlink(filePath, newFilePath)\n",
    "            elif os.path.isfile(testFilePath):\n",
    "                os.symlink(testFilePath, newFilePath)\n",
    "            elif os.path.isfile(validateFilePath):\n",
    "                os.symlink(validateFilePath, newFilePath)\n",
    "            elif os.path.isfile(trainFilePath):\n",
    "                os.symlink(trainFilePath, newFilePath)\n",
    "            else:\n",
    "                print(\"File missing: \", fileName)\n",
    "                \n",
    "symlink_classes_images(train, validate, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_glasses = train[['file_name', 'eyeglasses']]\n",
    "train_glasses = train_glasses.replace(to_replace=1, value=1).replace(to_replace=-1, value=0).reset_index(drop=True)\n",
    "validate_glasses = validate[['file_name', 'eyeglasses']]\n",
    "validate_glasses = validate_glasses.replace(to_replace=1, value=1).replace(to_replace=-1, value=0).reset_index(drop=True)\n",
    "test_glasses = test[['file_name', 'eyeglasses']]\n",
    "test_glasses = test_glasses.replace(to_replace=1, value=1).replace(to_replace=-1, value=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1947\n",
      "1     792\n",
      "Name: eyeglasses, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "n_glasses = train_glasses['eyeglasses'].value_counts()\n",
    "print(n_glasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#https://jovianlin.io/cat-crossentropy-vs-sparse-cat-crossentropy/\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(3, 128, 128)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "#model.add(LeakyReLU(alpha=0.3))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2739 images belonging to 2 classes.\n",
      "Found 913 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "def get_ImageDataGenerator(shear_range=0, \n",
    "                           zoom_range=0, \n",
    "                           horizontal_flip=False):\n",
    "    datagen = ImageDataGenerator(\n",
    "                rescale=1./255,\n",
    "                shear_range=0,\n",
    "                zoom_range=0,\n",
    "                horizontal_flip=False,\n",
    "                data_format='channels_first')\n",
    "    \n",
    "    return datagen\n",
    "\n",
    "def get_flow_from_directory(ImageDataGenerator,\n",
    "                            img_dir,\n",
    "                            target_size=(128,128),\n",
    "                            class_mode='binary',\n",
    "                            batch_size=32,\n",
    "                            shuffle=True,\n",
    "                            seed=123):\n",
    "    gen = ImageDataGenerator.flow_from_directory(\n",
    "                            directory=img_dir,\n",
    "                            target_size=target_size,\n",
    "                            class_mode=class_mode,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            seed=seed,\n",
    "                            follow_links=True)\n",
    "    return gen\n",
    "\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = get_ImageDataGenerator(shear_range=0,\n",
    "                                       zoom_range=0,\n",
    "                                       horizontal_flip=False)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = get_ImageDataGenerator()\n",
    "#Takes the dataframe and the path to a directory and \n",
    "#generates batches of augmented/normalized data.\n",
    "\n",
    "trainPath = './images/train/eyeglasses/'\n",
    "validatePath = './images/validate/eyeglasses/'\n",
    "batch_size = 32\n",
    "target_size=(128,128)\n",
    "class_mode='binary'\n",
    "shuffle=True\n",
    "seed=123\n",
    "\n",
    "train_gen = get_flow_from_directory(train_datagen,\n",
    "                                    trainPath,\n",
    "                                    target_size,\n",
    "                                    class_mode,\n",
    "                                    batch_size,\n",
    "                                    shuffle,\n",
    "                                    seed)\n",
    "\n",
    "validation_gen = get_flow_from_directory(test_datagen,\n",
    "                                    validatePath,\n",
    "                                    target_size,\n",
    "                                    class_mode,\n",
    "                                    batch_size,\n",
    "                                    shuffle,\n",
    "                                    seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0.001,\n",
    "                              patience=0,\n",
    "                              verbose=1, \n",
    "                              mode='auto',\n",
    "                              restore_best_weights=True)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', \n",
    "                       factor=0.2, \n",
    "                       patience=0, \n",
    "                       verbose=1, \n",
    "                       mode='auto', \n",
    "                       min_delta=0.01, \n",
    "                       cooldown=0, \n",
    "                       min_lr=0.001)\n",
    "train_gen.reset()\n",
    "validation_gen.reset()\n",
    "\n",
    "train_steps=train_gen.n/train_gen.batch_size\n",
    "valid_steps=validation_gen.n/validation_gen.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_steps, valid_steps)\n",
    "print(train_steps*train_gen.batch_size, valid_steps*validation_gen.batch_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(generator=train_gen,\n",
    "                    steps_per_epoch=train_steps,\n",
    "                    validation_data=validation_gen,\n",
    "                    validation_steps=valid_steps,\n",
    "                    epochs=50,\n",
    "                    callbacks=[earlyStopping, LR],\n",
    "                    class_weight = 'auto')\n",
    "\n",
    "#model.save_weights('.h5')\n",
    "model.save_weights('glasses_detection_from_dir.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_gen = get_flow_from_directory(test_datagen,\n",
    "                                        validatePath,\n",
    "                                        target_size,\n",
    "                                        class_mode,\n",
    "                                        1,\n",
    "                                        False)\n",
    "\n",
    "validation_gen.reset()\n",
    "\n",
    "model.evaluate_generator(generator=validation_gen, steps=913, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPath = './images/test/'\n",
    "\n",
    "\n",
    "test_gen = get_flow_from_directory(test_datagen,\n",
    "                                    testPath,\n",
    "                                    target_size,\n",
    "                                    class_mode,\n",
    "                                    1,\n",
    "                                    False)\n",
    "\n",
    "test_gen.reset()\n",
    "\n",
    "model.evaluate_generator(generator=test_gen, steps=913, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen.reset()\n",
    "probabilities = model.predict_generator(test_gen,verbose=1,steps=len(test_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "output_df = pd.DataFrame(\n",
    "    {'filename': test_gen.filenames,\n",
    "     'y_true': test_gen.classes,\n",
    "     'y_pred': list(map(int, np.rint(probabilities).flatten()))\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df_wrong = output_df.query('y_true != y_pred')\n",
    "output_df_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = test_gen.classes\n",
    "y_pred = np.rint(probabilities)\n",
    "confusion_matrix(y_true=y_true, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AMLSassignment",
   "language": "python",
   "name": "amlsassignment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
