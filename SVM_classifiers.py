import csv
import os
import pickle
import sys

import numpy as np
import pandas as pd
from PIL import Image
from scipy.stats import expon
from sklearn import metrics, svm
from sklearn.decomposition import PCA
from sklearn.model_selection import RandomizedSearchCV

from CNN_functions import split_dataset_random
from CNN_classifiers import get_metrics
from ImageFiltering import get_features


def prepare_for_svm(seed=0):

    """
    prepare_for_svm is a function that reads a filtered image dataset dataframe from csv, 
    which is generated by ImageFiltering.build_filtered_dataframe.
    The function then splits this filtered dataframe into randomized train and test sets
    in preparation for training the SVM classifier.

    The validation set is not used here because cross-validation is performed on the SVM with the training set.

    If the 'seed' parameter is kept at its default value (0), the test split will be identical to the 
    one used to evaluate the custom CNN architecture classifier.

    Parameters:
    'seed': The seed to use for the randomized train and test split.

    Returns:
    train, validate, test: The randomized split sets as Dataframes
    """

    filteredDf = pd.read_csv("./generated_csv/attribute_list_filtered.csv")
    # split_dataset_random is a function from CNN_functions.py, will use a
    train, validate, test = split_dataset_random(
        df=filteredDf, validation_split=0, test_split=0.2, seed=seed
    )
    return train, validate, test


def create_image_dataset(dataframe, images_path="./images/"):

    '''
    create_image_dataset creates a dictionary of image RGB value arrays with a shape of (128, 128, 3), keyed by the image number.
    It extracts image arrays for all the images in the provided dataframe.

    Parameters:
    dataframe: Dataframe of the images in your dataset, must have a 'file_name' column with image numbers.
    images_path: the path to the directory where your image dataset is.

    Returns:
    img_arrays: Dictionary of RGB image arrays, keyed by the image number.

    '''

    img_arrays = {}
    for i, image in enumerate(dataframe["file_name"]):

        img_path = images_path + str(image) + ".png"
        #Read image data as RGB with shape (128,128,3):
        img = Image.open(img_path).convert("RGB").resize((128, 128))
        #Reshape 3D array to 2D, reshaping results in image shape (128, 128*3):
        img_array = np.array(img).reshape(128, -1)
        #Scale RGB values to be between 0 and 1:
        img_array = (img_array - img_array.mean()) / np.sqrt(img_array.var() + 1e-5)
        #add img_array to img_arrays dict keyed by image number:
        img_arrays[image] = img_array
        print(i)

    print(sys.getsizeof(img_arrays))
    return img_arrays


def get_X_Y_from_dataset(image_dataset, row_name=None, labels_dataframe=None, unlabelled=False):
    
    '''
    get_X_Y_from_dataset further splits a dataframe into two arrays, X and Y.
    X contains the image RGB values, which are processed with PCA to have lowered dimensionality (128, 1).
    Y contains the corresponding image class labels from the chosen row_name.

    Parameters:
    image_dataset: Dictionary of image data, keyed by image number. Returned by create_image_dataset.
    row_name: row_name on which the classifier will be trained, used to select which row of labels dataframe to use for Y.
    labels_dataframe: Dataframe of all the image numbers and their class labels.

    Returns:
    X: numpy array of image RGB data scaled to be between 0 and 1 and processed with PCA
    Y: numpy array of corresponding image labels
    '''
    
    #Use sklearn PCA function with a fixed n_components=128 for all images.
    pca = PCA(n_components=128)
    X = []  # images
    Y = []  # labels
    for key, value in image_dataset.items():
        value_PCA = pca.fit_transform(value)

        X.append(value_PCA)
        if unlabelled==False:
            df = labels_dataframe
            #gets the class label for the current key (file number) from the supplied labels_dataframe:
            label = df.loc[df["file_name"] == key, row_name].item()
            if label == -1:
                label = 0
            Y.append(label)

    #convert X Y lists to arrays
    X = np.array(X)
    Y = np.array(Y)
    return X, Y


def run_svm_classifier(
    row_name, model_save_load_path, images_path="./images/", load_model=False
):

    '''
    run_svm_classifier, trains and evaluates the Linear SVM classifier on the filtered dataset.
    Randomized search with 3 fold cross validation is used to tune the 'C' parameter.
    Generates a csv (e.g. task_4_svm.csv) that contains the test accuracy and test predictions for each image.
    The trained model is saved as a Pickle file (these take up more than 100MB of space each)

    Parameters:
    row_name: row name of the task on which to run the classifcation ('human', 'eyeglasses', 'hair_color', 'smiling', 'young')
    model_save_load_path: path to directory where to save or load the trained model pickle file.
    images_path: path to the image dataset directory.
    load_model: Boolean, whether to load a trained model from pickle file or train from scratch.
    '''
    
    model_path = model_save_load_path + row_name + "_svm_trained.sav"
    #split filtered dataset into train, validate and test sets:
    train, validate, test = prepare_for_svm(seed=0)

    #get flattened versions of all the images RGB data:
    image_dataset_train = create_image_dataset(train, images_path=images_path)
    image_dataset_test = create_image_dataset(test, images_path=images_path)
    test_unlabelled = pd.DataFrame(range(0,100),columns=['file_name'])
    image_dataset_unlabelled_test = create_image_dataset(test_unlabelled, images_path='./testing_dataset/flow_subdir/')
    X_train, Y_train = get_X_Y_from_dataset(image_dataset_train, row_name, train)
    X_test, Y_test = get_X_Y_from_dataset(image_dataset_test, row_name, test)
    #X_test_unlabelled is the unlabelled test image data, Y_test_unlabelled is blank:
    X_test_unlabelled, Y_test_unlabelled = get_X_Y_from_dataset(image_dataset_unlabelled_test)
    n_train, nx_train, ny_train = X_train.shape
    n_test, nx_test, ny_test = X_test.shape
    n_test_u, nx_test_u, ny_test_u = X_test_unlabelled.shape
    X_train_flat = X_train.reshape((n_train, nx_train * ny_train))
    X_test_flat = X_test.reshape((n_test, nx_test * ny_test))
    X_test_u_flat = X_test_unlabelled.reshape((n_test_u, nx_test_u * ny_test_u))

    if load_model == False:

        classifier = get_svm_classifier(randomized_search=True, n_iter=5)

        print("Fitting classifier...")

        classifier.fit(X_train_flat, Y_train)

        if os.path.isdir(model_save_load_path) == False:
            os.makedirs(model_save_load_path)

        pickle.dump(classifier, open(model_path, "wb"))
    else:
        classifier = pickle.load(open(model_path, "rb"))

    y_pred = classifier.predict(X_test_flat)
    filenames = test["file_name"]

    output_df, output_df_wrong, report, accuracy, confusion = get_metrics(
        Y_test, y_pred, filenames
    )
    generate_test_predictions_csv(output_df, accuracy, row_name, path_append="svm")

    y_pred_u = classifier.predict(X_test_u_flat)
    filenames_u = image_dataset_unlabelled_test['file_name']
    output_df_u = pd.DataFrame(
        {"filename": filenames_u, "y_pred": y_pred_u}
    )
    generate_unlabelled_test_predictions_csv(
        output_df_u, row_name, path_append="SVM"
    )


def get_svm_classifier(randomized_search=True, n_iter=10):
    '''
    
    '''
    svc = svm.SVC(gamma="scale", kernel="linear", verbose=True, class_weight="balanced")
    if randomized_search:
        parameters = {"C": expon(scale=100)}
        classifier = RandomizedSearchCV(
            svc, parameters, n_iter=n_iter, verbose=1, n_jobs=-1
        )
    else:
        classifier = svc
    return classifier


def generate_test_predictions_csv(
    output_df, accuracy, row_name, save_dir='./csv_predictions/', path_append="ResNet50"
):
    output_df["sort"] = output_df["filename"].astype(int)
    output_df = output_df.sort_values("sort", ascending=True).reset_index(drop=True)
    output_df = output_df.drop("sort", axis=1)

    # Change all '0' values in y_pred to '-1', to match the format of y_true (-1 and 1):
    # only for binary
    if row_name != 'hair_color':
        output_df.loc[output_df.y_pred == 0, "y_pred"] = -1
        output_df.loc[output_df.y_true == 0, "y_true"] = -1

    # generate csv_filename:
    if path_append == None:
        csv_filename = row_name + '_pred' + ".csv"
    else:
        csv_filename = row_name + '_pred' + "_" + path_append + ".csv"

    with open(save_dir+csv_filename, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow([accuracy])
        for i in range(len(output_df)):
            writer.writerow([output_df["filename"][i], output_df["y_pred"][i], output_df['y_true'][i]])

def generate_unlabelled_test_predictions_csv(
    output_df, row_name, path_append="ResNet50"
):
    output_df["sort"] = output_df["filename"].astype(int)
    output_df = output_df.sort_values("sort", ascending=True).reset_index(drop=True)
    output_df = output_df.drop("sort", axis=1)

    if row_name == "smiling":
        task_num = 1
    elif row_name == "young":
        task_num = 2
    elif row_name == "eyeglasses":
        task_num = 3
    elif row_name == "human":
        task_num = 4
    elif row_name == "hair_color":
        task_num = 5
    csv_filename = "task_" + str(task_num) + "_" + path_append + ".csv"

    with open(csv_filename, "w", newline="") as f:
        writer = csv.writer(f)
        for i in range(len(output_df)):
            writer.writerow([output_df["filename"][i], output_df["y_pred"][i]])


"""https://arxiv.org/abs/1403.6382"""


def transfer_learning_svm(row_name, load_features=False):
    filteredDf = pd.read_csv("./generated_csv/attribute_list_filtered.csv")
    # Drop rows where hair_color = -1, these are mislabeled and impact the accuracy of the trained model
    if row_name == "hair_color":
        filteredDf = filteredDf[filteredDf.hair_color != -1]

    if load_features == False:
        feature_df = get_features(
            img_df=filteredDf,
            network="resnet50",
            pooling="max",
            images_path="./images",
            df_to_pickle=True,
            pickle_save_dir="generated_csv/transfer_learning/",
        )
    else:
        feature_df = pd.read_pickle(
            "generated_csv/transfer_learning/image_features_resnet50_max.pkl"
        )

    filtered_feature_df = pd.merge(
        filteredDf, feature_df, left_on="file_name", right_on="id"
    )
    print(filtered_feature_df.head())

    train, validate, test = split_dataset_random(
        df=filtered_feature_df, validation_split=0, test_split=0.2, seed=0
    )
    print("YTRAIN SHAPE: " + str(train["features"].shape))

    classifier = get_svm_classifier(randomized_search=True)
    train_feature_list = np.array(train["features"].tolist())
    test_feature_list = np.array(test["features"].tolist())
    train_feature_list = train_feature_list.reshape((train_feature_list.shape[0], -1))
    test_feature_list = test_feature_list.reshape((test_feature_list.shape[0], -1))
    print("Test_feature_list shape: ", str(test_feature_list.shape))
    print("train_feature_list shape: ", str(train_feature_list.shape))
    print("LABEL SHAPE: ", str(train[row_name].shape))
    classifier.fit(train_feature_list, train[row_name])

    y_pred = classifier.predict(test_feature_list)
    y_true = test[row_name].values

    print("y_true shape:", str(y_true.shape))
    filenames = test["file_name"].values

    print("y_pred shape: ", y_pred.shape)
    print("filenames shape: ", filenames.shape)

    output_df, output_df_wrong, report, accuracy, confusion = get_metrics(
        y_true, y_pred, filenames
    )
    generate_test_predictions_csv(output_df, accuracy, row_name, path_append="resnet50")

    test_u_df = pd.DataFrame({'file_name':range(1,101)})
    feature_u_df = get_features(
            img_df=test_u_df,
            network="resnet50",
            pooling="max",
            images_path="./testing_dataset/flow_subdir",
            df_to_pickle=False,
            pickle_save_dir="generated_csv/transfer_learning/",
        )
    feature_u_list = np.array(feature_u_df["features"].tolist())
    feature_u_list = feature_u_list.reshape((feature_u_list.shape[0], -1))
    y_pred_u = classifier.predict(feature_u_list)

    filenames_u = test_u_df['file_name']
    output_u_df = pd.DataFrame(
        {"filename": filenames_u, "y_pred": y_pred_u}
    )

    generate_unlabelled_test_predictions_csv(
        output_u_df, row_name, path_append="ResNet50"
    )
